\chapter{Evaluation}

This chapter evaluates Storm-MC. We describe the metrics used to evaluate performance of Storm-MC (\ref{sec:metrics}), list the configuration used for benchmarking (\ref{sec:system_conf}), compare Storm-MC to Apache Storm executing in local mode on a set of different topologies (\ref{sec:performance}), and finally talk about challenges encountered while building Storm-MC (\ref{sec:challenges}).

\section{Evaluation Metrics}
\label{sec:metrics}

The performance of Apache Storm and Storm-MC was evaluated on the following three metrics:

\begin{description}
	\item[Throughput] \hfill \\
	The tuple throughput per component was computed over a period of five minutes. This metric provides an insight into implementation performance of the evaluated systems.
	\item[CPU Utilisation] \hfill \\
	The average CPU utilisation was computed by sampling the instant CPU utilisation every second throughout program execution. This metric shows how utilised the machine was while running a topology with the same configuration on both systems.
	\item[Resident Memory Size] \hfill \\
	The average resident size was computed by sampling the instant resident size every second throughout program execution. This metric provides an insight into how much main memory both systems consume to run a topology with the same configuration.
\end{description}

\section{System Configuration}
\label{sec:system_conf}

\subsection{Software Setup}

All performance benchmarks in this chapter were executed on the following versions of Apache Storm and Storm-MC:

\begin{itemize}
	\item Apache Storm version 0.9.2 \footnote{\url{https://github.com/mrknmc/storm/releases/v0.9.2-fix}}
	\item Storm-MC version 0.1.6 \footnote{\url{https://github.com/mrknmc/storm-mc/releases/0.1.6}}
\end{itemize}

The Apache Storm source code had to be adapted to include a workaround for a deadlock bug present in version 0.9.2. This bug caused a topology to exit with threads left in Zombie state under certain conditions. This prevented Storm from logging the benchmark metrics after execution. Hence a workaround was added so the metrics were logged.

Version 0.1.6 is the latest version of Storm-MC as of this moment. The first release was version 0.1.0 which was production-ready but since then there were 6 minor versions fixing bugs as they were discovered during testing.

Furthermore, the following versions of benchmark libraries were used to implement the performance benchmarks described in this chapter:

\begin{itemize}
	\item A fork of IBM Storm Email Benchmarks version 0.1.12 \footnote{\url{https://github.com/mrknmc/benchmarks/releases/0.1.12}}
	\item Storm-benchmark version 0.1.0 \footnote{\url{https://github.com/manuzhang/storm-benchmark}}
\end{itemize}

IBM open sourced a project with a suite of benchmarks which they used to compare Apache Storm to their real-time data stream system IBM Infosphere Streams \citep{InfoSphereStreams}. We forked this project and used the Enron topology to benchmark Storm-MC against Apache Storm. The current version of the fork is 0.1.12.

Apache Storm developers use a project called storm-benchmark when testing the performance of Apache Storm. The FileReadWordCount and RollingSort topologies from this project were adapted and used to benchmark Storm-MC against Apache Storm. 

Since Storm-MC reuses package names from Apache Storm, the same benchmark is directly executable by both libraries. This way there was no need to maintain two different benchmarks suites and it can be easily shown that the same code was executed for both systems. An example topology submission to Storm-MC and Apache Storm, respectively, would look as follows:


\todo{submission with shell script which resolves to something else.}

\begin{minted}{bash}
storm-mc jar storm-email-benchmark.jar
    com.ibm.streamsx.storm.email.benchmark.FileReadWordCount wordcount

storm jar storm-email-benchmark.jar
    com.ibm.streamsx.storm.email.benchmark.FileReadWordCount wordcount
\end{minted}

\begin{minted}{bash}
java -cp storm-multicore-0.1.6.jar storm-email-benchmark.jar
    ...
    com.ibm.streamsx.storm.email.benchmark.FileReadWordCount wordcount
java -cp storm-core-0.9.2-incubating.jar storm-email-benchmark.jar
    ...
    com.ibm.streamsx.storm.email.benchmark.FileReadWordCount wordcount
\end{minted}

There are several things being done in this one command:

\begin{itemize}
	\item The JAR file of the corresponding library is added to the Java classpath.
	\item The JAR file of the benchmark library is added to the Java classpath.
	\item The main class is specified.
	\item The name of the topology is defined as an argument.
\end{itemize}

In this example the Storm home directory was set, the corresponding library was added to the Java classpath, the JAR file containing the benchmark was submitted, and the main class and topology name were specified.

\subsection{Hardware Setup}

The machine used for benchmarking is the Informatics Student Compute server (\texttt{student.compute.inf.ed.ac.uk}). The server has the following hardware components:

\begin{description}
	\item[Processor: Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} E5-2690 v2 @ 3.00 GHz] \hfill \\
	The machine has two sockets with the same processor each. The processor has 10 physical cores with Hyper-Threading Technology which means it can handle up to 20 threads in parallel. Thus with two sockets, there is potential to execute up to 40 threads in parallel.
	\item[Main Memory] \hfill \\
	The machine has 378 GB of main memory. Since data stream processing uses windows to store only up to a certain amount of memory this was more than enough to conduct the benchmarks.
\end{description}

\subsection{Storm Configuration}

As mentioned before, when submitting a topology the programmer needs to submit a configuration file as well. To ensure that the performance difference between Apache Storm and Storm-MC was not caused by different configuration, the default configuration file from Storm 0.9.2 was used to benchmark both projects. Most notably, the size of the ring buffer used by executors \\ (\texttt{topology.executor.receive.buffer.size}) was set to 1024 and the wait strategy employed by executors when there are no tuples to pick up \\ (\texttt{topology.disruptor.wait.strategy}) was set to \texttt{BlockingWaitStrategy}.

\section{Results}
\label{sec:performance}

To assess performance of Storm-MC, 3 different benchmarks were executed, each with a different focus. The benchmarks were executed for a constant period of time - five minutes - after which the system was killed and metrics were collected. To avoid any performance differences caused by varying amounts of heap memory required by the tested systems, the programs were run with the following flag: \texttt{-Xmx10240M}. This flag sets the maximum amount of heap memory used by the JVM to 10 GB which was more than enough for all benchmarks.

The parallelism of components was varied from one to six and average CPU utilisation and resident memory size were recorded by the Unix \texttt{top} program \citep{UnixTop}. Maximum CPU utilisation with 40 threads is 4,000\%. Resident memory size is the amount of non-swapped physical memory a task has used. This metric can be deceiving as it depends on how OS manages memory but it is the only fairly reliable memory metric reported by \texttt{top} that can be used for Java programs. We only employ this metric to show proportional difference in memory usage between Storm and Storm-MC.

\subsection{WordCount Topology}

The first topology tested for performance is a variant of the aforementioned WordCount topology. This topology has a spout \texttt{FileReadSpout} generating random sentences, which sends messages to a \texttt{SplitSentenceBolt} bolt which splits the sentences on whitespace and sends individual words to a \texttt{CountBolt} which counts word frequencies. Recall, that this topology is shown graphically in Figure \ref{fig:wordcount_topology}. Since the components do not store any data in memory or make any I/O calls this topology is mostly CPU-bound.

The number of tuples processed by each component in Storm-MC and Apache Storm is shown in tables \ref{table:storm_mc_wordcount} and \ref{table:storm_wordcount}, respectively. As can be seen from the tables, not only was CPU utilisation in Storm-MC lower, Storm-MC often processed more than twice as many tuples per component than Apache Storm.  The number of tuples processed by \texttt{CountBolt}, the last component of the topology, is also show in Figure \ref{fig:countbolt-plot}. Since this topology is serial, the number of tuples processed by \texttt{CountBolt} is a good indicator of total throughput.

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rccccl@{}}
    \textbf{Parallelism} & \textbf{FileReadSpout} & \textbf{SplitSentenceBolt} & \textbf{CountBolt} & \textbf{CPU Utilisation} & \textbf{Resident Size} \\ \toprule
    1 & {25,767,502} & {25,767,502} & {225,815,174} & {217.9\%} & {690.8M} \\
    2 & {34,403,678} & {34,403,127} & {301,493,247} & {414.6\%} & {759.1M} \\
    3 & {45,731,188} & {45,732,988} & {400,767,999} & {611.5\%} & {798.4M} \\
    4 & {52,285,327} & {52,283,540} & {458,187,555} & {805.5\%} & {804.1M} \\
	5 & {55,326,941} & {55,325,167} & {484,844,652} & {998.7\%} & {806.0M} \\
	6 & {56,747,319} & {56,744,629} & {497,285,149} & {1,195.3\%} & {824.8M} \\
	7 & {51,048,092} & {51,044,774} & {447,330,207} & {1,314.8\%} & {686.5M} \\
	8 & {51,892,349} & {51,889,982} & {454,739,674} & {1,502.0\%} & {687.1M} \\
	9 & {55,312,654} & {55,308,442} & {484,696,841} & {1,656.8\%} & {694.9M} \\
	10 & {54,377,002} & {54,374,432} & {476,512,544} & {1,822.5\%} & {702.0M} \\
%	10 & {40,341,798} & {40,336,962} & {353,490,567} & {1,967.4\%} & {2.7G} \\
%	20 & {55,442,981} & {55,430,441} & {485,763,957} & {3161.5\%} & {2.1} \\
\end{tabular}
\caption{Storm-MC: Component throughput in WordCount Topology.}
\label{table:storm_mc_wordcount}
\end{adjustwidth}
\end{table}

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rccccl@{}}
    \textbf{Parallelism} & \textbf{FileReadSpout} & \textbf{SplitSentenceBolt} & \textbf{CountBolt} & \textbf{CPU Utilisation} & \textbf{Resident Size} \\ \toprule
    1 & {12,583,377} & {12,579,132} & {110,233,966} & {294.5\%} & {2.2G} \\
    2 & {16,800,475} & {16,796,695} & {147,194,709} & {481.7\%} & {2.8G} \\
    3 & {22,120,695} & {22,107,696} & {193,735,106} & {687.1\%} & {2.6G} \\
    4 & {20,720,637} & {20,711,756} & {181,500,586} & {895.3\%} & {2.6G} \\
	5 & {17,177,688} & {17,164,209} & {150,412,037} & {1,129.3\%} & {2.5G} \\
	6 & {17,402,418} & {17,388,691} & {152,374,303} & {1,342.1\%} & {2.3G} \\
	7 & {17,702,568} & {17,689,940} & {155,009,826} & {1,532.6\%} & {2.4G} \\
	8 & {19,161,174} & {19,143,254} & {167,749,429} & {1,697.3\%} & {2.6G} \\
	9 & {17,833,631} & {17,803,323} & {156,006,619} & {1,903.4\%} & {2.7G} \\
	10 & {17,488,484} & {17,442,562} & {152,843,992} & {2,136.7\%} & {2.8G} \\
%	10 & {12,229,523} & {12,211,100} & {107,002,632} & {2,136.7\%} & {2.8G} \\
%	20 & {19,950,916} & {19,892,159} & {174,299,740} & {2,813.7\%} & {3.5G} \\
\end{tabular}
\caption{Apache Storm: Component throughput in WordCount Topology.}
\label{table:storm_wordcount}
\end{adjustwidth}
\end{table}

<<countbolt-plot, echo=FALSE, cache=FALSE, fig.cap="CountBolt throughput in Apache Storm and Storm-MC", fig.align="center", message=F, warning=F, fig.pos="!htb", fig.height=3.5>>=
@

Furthermore, it can be seen that after the parallelism is increased beyond three the throughput of Apache Storm tails off and starts going down. This can be attributed to the number of threads ran by Apache Storm. For Storm-MC this tailing off occurs with parallelism of six where the overhead of multiple producers possibly trying to publish to the same queue becomes apparent. Moreover, with parallelism set to 6, Storm-MC executes with 20 threads which is close to the number of physical cores of the machine. However, it should be noted that even with parallelism equal to 10, Storm-MC still processes more than three times as many tuples as Storm.

<<threads-plot, echo=FALSE, cache=FALSE, fig.cap="Number of threads used by Apache Storm and Storm-MC", fig.align="center", fig.pos="!htb", fig.height=3.5>>=
@

The number of threads required to execute a topology is a linear function of the parallelism for both Storm and Storm-MC. However, as shown in Figure \ref{fig:threads-plot}, the number of threads required by Storm increases more rapidly than Storm-MC. For example, with parallelism set to 10, Storm creates 109 threads whereas Storm-MC creates only 32. More formally, the number of threads required by both systems can be expressed as:

\begin{figure}[!htb]
\begin{eqnarray*}
	49 \ + \ 2 \times \sum\limits_{c}^{components} parallelism(c) && \text{ for Apache Storm.} \\
	2 \ + \ \sum\limits_{c}^{components} parallelism(c) && \text{ for Storm-MC.}
\end{eqnarray*}
\caption{Number of threads used by Apache Storm and Storm-MC.}
\end{figure}

\textbf{N.B.}\@\xspace: This is a general formula that applies to all topologies, not just WordCount.

Of note, the resident size used by Storm-MC is also less than half of the resident size used by Storm for cases with parallelism less than 10.

\subsection{Enron Topology}

Next, Enron topology from the IBM benchmarks was tested for performance. In this topology, serialised emails from the Enron email dataset are read from a file by a \texttt{ReadEmailsDecompressSpout} spout. They are then deserialised by \texttt{AvroDeserializeBolt} bolt, filtered by \texttt{NewFilterBolt} bolt, modified by \texttt{ModifyBolt} bolt, and finally metrics are recorded by a \texttt{NewMetricsBolt} bolt. Additionally, every instance of the \texttt{NewMetricsBolt} bolt sends its local average email throughput to a global (excluded from the parallelism setting) \texttt{GlobalMetricsBolt} bolt every four seconds. This bolt then records the global average email throughput. Figure \ref{fig:enron_topology} shows this topology graphically.

\begin{figure}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
	\centering
	\includegraphics[scale=0.475]{pdf/enron_topology.pdf}
	\caption{Enron topology.}
	\label{fig:enron_topology}
\end{adjustwidth}
\end{figure}

Similarly to the WordCount topology, this topology is serial in nature. However, whereas the spout in  WordCount topology keeps a small number of sentences in memory, the Enron topology has a spout that produces tuples by reading from a file. Thus, this benchmark is mostly I/O-bound. The average email throughput in Storm-MC and Apache Storm is shown in tables \ref{table:storm_mc_enron} and \ref{table:storm_enron}, respectively. 

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rccl@{}}
    \textbf{Parallelism} & \textbf{Emails Processed} & \textbf{CPU Utilisation} & \textbf{Resident Size} \\ \toprule
    1 & {3,285,742} & {297.7\%} & {806.8M} \\
    2 & {6,696,612} & {482.1\%} & {756.1M} \\
    3 & {8,493,772} & {729.5\%} & {341.4M} \\
    4 & {11,102,969} & {1036.9\%} & {326.0M} \\
    5 & {12,630,475} & {1311.0\%} & {260.8M} \\
    6 & {14,082,501} & {1590.3\%} & {334.0M} \\
\end{tabular}
\caption{Storm-MC: Email Throughput in Enron Topology.}
\label{table:storm_mc_enron}
\end{adjustwidth}
\end{table}

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rccl@{}}
    \textbf{Parallelism} & \textbf{Emails Processed} & \textbf{CPU Utilisation} & \textbf{Resident Size} \\ \toprule
    1 & {2,943,709} & {406.6\%} & {1.94G} \\
    2 & {4,832,874} & {945.1\%} & {2.93G} \\
    3 & {5,623,028} & {1,427.4\%} & {3.32G} \\
    4 & {6,238,395} & {1,891.2\%} & {3.56G} \\
    5 & {6,105,155} & {2167.4\%} & {3.65G} \\
    6 & {7,242,298} & {2388.6\%} & {4.09G} \\
\end{tabular}
\caption{Apache Storm: Email Throughput in Enron Topology.}
\label{table:storm_enron}
\end{adjustwidth}
\end{table}

<<enron-plot, echo=FALSE, cache=FALSE, fig.cap="Global email throughput over time with standard error.", fig.align="center", fig.pos="!htb", fig.height=6.5>>=
@

As can be seen from the tables, the difference in throughput in Enron Topology is less staggering than in WordCount. This is due to the fact that the throughput is limited by the file reads of the spout. However, as the parallelism increases the improvement in throughput of Storm-MC becomes more apparent as shown in Figure \ref{fig:enron-plot}. This figure also shows that the throughput is fairly volatile. This is due to the fact that the file is loaded into main memory in chunks and hence the throughput drops when the spout is trying to read from a file in between the loads. As before, the resident size used by Storm-MC is significantly lower than that of Storm.

\subsection{RollingSort Topology}

The RollingSort topology was ported over from the aforementioned storm-benchmark project. This topology only includes one spout sending tuples to one bolt. The \texttt{RandomMessageSpout} spout produces hundred character-long strings of random digits from zero to eight. The \texttt{SortBolt} bolt then stores a rolling window of hundred of such messages and sorts them every 10 seconds. The graphic representation of this topology can be seen in Figure \ref{fig:rolling_topology}.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.475]{pdf/rolling_topology.pdf}
	\caption{RollingSort topology.}
	\label{fig:rolling_topology}
\end{figure}

This benchmark is considered to be memory-bound: the bolt stores a window of tuples in memory and performs a non-linear time sort. The results of running this benchmark on Storm-MC and Apache Storm can be seen in tables \ref{table:storm_mc_rolling} and \ref{table:storm_rolling}, respectively.

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rccccl@{}}
    \textbf{Parallelism} & \textbf{RandomMessageSpout} & \textbf{SortBolt} & \textbf{CPU Utilisation} & \textbf{Memory Usage} \\ \toprule
    1 & {249,143,444} & {249,142,400} & {186.2\%} & {504.3M} \\
    2 & {444,261,351} & {444,259,400} & {352.0\%} & {401.7M} \\
    3 & {350,861,061} & {350,859,800} & {514.7\%} & {382.9M} \\
    4 & {412,429,850} & {412,428,600} & {675.2\%} & {314.2M} \\
    5 & {470,813,184} & {470,811,300} & {835.8\%} & {423.2M} \\
    6 & {498,957,255} & {498,954,600} & {989.6\%} & {235.1M} \\
    7 & {519,744,352} & {519,741,700} & {1,149.0\%} & {637.9M} \\
    8 & {532,285,376} & {532,283,800} & {1,302.9\%} & {618.1M} \\
    9 & {501,519,539} & {501,517,700} & {1,430.4\%} & {579.4M} \\
    10 & {555,468,830} & {555,467,000} & {1,651.6\%} & {564.7M} \\ 
\end{tabular}
\caption{Storm-MC: Component throughput in RollingSort Topology.}
\label{table:storm_mc_rolling}
\end{adjustwidth}
\end{table}

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rccccl@{}}
    \textbf{Parallelism} & \textbf{RandomMessageSpout} & \textbf{SortBolt} & \textbf{CPU Utilisation} & \textbf{Memory Usage} \\ \toprule
    1 & {173,906,935} & {173,900,300} & {267.3\%} & {3.0G} \\
    2 & {226,583,924} & {226,579,200} & {468.3\%} & {3.0G} \\
    3 & {310,949,455} & {310,943,000} & {634.6\%} & {2.9G} \\
    4 & {362,675,336} & {362,663,600} & {815.2\%} & {2.8G} \\
    5 & {409,470,032} & {409,462,100} & {969.4\%} & {2.7G} \\
    6 & {435,471,042} & {435,459,600} & {1,139.6\%} & {2.6G} \\
    7 & {395,386,336} & {395,309,900} & {1,327.7\%} & {2.6G} \\
    8 & {366,680,402} & {366,553,300} & {1,509.1\%} & {2.7G} \\
    9 & {359,091,633} & {358,942,200} & {1,689.5\%} & {2.7G} \\
    10 & {313,912,451} & {313,811,300} & {1,889.5\%} & {2.7G} \\
\end{tabular}
\caption{Apache Storm: Component throughput in RollingSort Topology.}
\label{table:storm_rolling}
\end{adjustwidth}
\end{table}

While Storm-MC still outperforms Apache Storm, the difference in performance is marginal. This is due to the fact that the topology only has two components and the application is mostly memory bound. Storm-MC provides speed improvements for topologies that are mostly CPU-bound and have several components working serially such as WordCount. Furthermore, Storm-MC beats Storm significantly when the parallelism is high, as in previous topologies.

\section{Challenges}
\label{sec:challenges}

In this section we discuss challenges we encountered while porting Apache Storm to multi-core. We also try to provide a critical analysis of the project.

\begin{description}
	\item[Backwards Compatibility] \hfill \\
	A big challenge while working on this project was ensuring that the final system is backwards compatible with Apache Storm. Doing this ensured that existing applications developed for Apache Storm can be executed with Storm-MC. On the other hand, this was sometimes limiting the possible performance improvements.
	\item[Unfamiliarity with Clojure] \hfill \\
	One of the main challenges while working on this project was learning a new programming language - Clojure. Since most of the implementation of Apache Storm is written in Clojure, this language had to be studied and its concepts well understood for us to be able to write code that worked with the existing codebase. By the end of the project writing Clojure has become second nature to us but initially progress was slow.
	\item[Lack of Documentation] \hfill \\
	Even though Apache Storm is a popular project documentation is available only for the high level concepts used within Storm. The implementation details are often obscured away in hard to understand functions. Since the documentation is lacking our knowledge of Storm had to be obtained by reading the source code of an initially unfamiliar language. By the end of the project the Storm-MC code became well documented and we might attempt back-porting to Apache Storm.
\end{description}

