\chapter{Evaluation}

In this chapter we evaluate Storm-MC. We describe the metrics used to evaluate performance of Storm-MC (\ref{sec:metrics}), list the configuration used for benchmarking (\ref{sec:system_conf}), compare Storm-MC to Apache Storm executing in local mode on a set of different topologies (\ref{sec:performance}), and finally talk about challenges encountered while designing Storm-MC (\ref{sec:challenges}).

\section{Evaluation Metrics}
\label{sec:metrics}

The system was evaluated on the following metrics:

\begin{description}
	\item[Throughput] \hfill \\
	The number of tuples processed by every component in the given time of the topology is recorded.
	\item[CPU utilisation] \hfill \\
	Usage of CPU is recorded every \textbf{x} seconds throughout execution and is then averaged.
	\item[Memory utilisation] \hfill \\
	Main memory usage is recorded every \textbf{x} seconds throughout execution and is then averaged.
\end{description}

\section{System Configuration}
\label{sec:system_conf}

\subsection{Software Setup}

All performance benchmarks were ran using the following software packages:

\begin{itemize}
	\item Apache Storm version 0.9.2
	\item Storm-MC version 0.1.6
	\item A fork of IBM Storm Email Benchmarks version 0.1.10
	\item Storm-benchmark version 0.1.0
\end{itemize}

The Apache Storm source code had to be adapted to include a workaround for a deadlock bug present in version 0.9.2. This bug caused a topology to exit with threads left in Zombie state under certain conditions. This prevented Storm from logging the benchmark metrics after execution. Hence a workaround was added so the results were logged.

Version 0.1.6 is the latest version of Storm-MC as of this moment. The first release was version 0.1.0 which was production-ready but since then there were 6 minor versions fixing bugs as they were discovered during testing.

IBM open sourced a suite of benchmarks which they used to compare Apache Storm to their real-time stream system IBM Infosphere Streams \citep{InfoSphereStreams}. These benchmarks were adapted and used to benchmark Storm-MC against Apache Storm.

Lastly, a number of spout and bolt components were used from the storm-benchmark project which Apache Storm developers use to benchmark Storm.

Since Storm-MC reuses package names from Apache Storm, the same benchmark is directly executable by both libraries. This saved a lot of time and furthermore there is no need to maintain two benchmarks suites.

\todo{Go into more detail which components were re-used and where?}

\subsection{Hardware Setup}

The machine used for benchmarking is the Informatics Student Compute server (\texttt{student.compute.inf.ed.ac.uk}). The server has the following hardware components:

\begin{description}
	\item[Processor: Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} E5-2690 v2 @ 3.00 GHz] \hfill \\
	The machine has two sockets with the same processor each. The processor has 10 physical cores with Hyper-Threading Technology which means it can handle up to 20 threads in parallel. Thus with two sockets, there is potential to execute up to 40 threads in parallel.
	\item[Main Memory] \hfill \\
	The machine has 378 GB of main memory. Since data stream processing uses windows to store only up to a certain amount of memory this was more than enough to conduct the benchmarks.
\end{description}

\section{Performance}
\label{sec:performance}

To assess performance of Storm-MC, 4 different benchmarks were executed, each with a different focus. The benchmarks were executed for a constant period of time (five minutes) after which the system was killed and metrics were collected. To avoid any performance differences caused by varying amounts of heap memory required by the tested systems, the programs were run with the following flag: \texttt{-Xmx10240M}. This flag sets the maximum amount of heap memory used by the JVM to 10 GB which was more than enough for all benchmarks.

The parallelism of components was varied from one to six and average CPU utilisation and resident memory size were recorded by the Unix \texttt{top} program \citep{UnixTop}. Maximum CPU utilisation with 40 threads is 4,000\%. Resident memory size is the amount of non-swapped physical memory a task has used. This metric can be deceiving as it depends on how OS manages memory but it is the only fairly reliable memory metric reported by \texttt{top} that can be used for Java programs. This is because the amount of virtual memory used by a Java program can be skewed by the JVM reserving memory before needing it.

\subsection{WordCount Topology}

The first topology we tested for performance is a variant of the aforementioned WordCount topology. Recall, that this topology is shown graphically on figure \ref{fig:wordcount_topology}. Since the components do not store any data in memory or make any I/O calls this topology is considered to be mostly CPU-bound.

The number of tuples processed by each component in Storm-MC and Apache Storm is shown in tables \ref{table:storm_mc_wordcount} and \ref{table:storm_wordcount}, respectively. As can be seen from the tables, not only was CPU utilisation in Storm-MC lower, Storm-MC often processed more than twice as many tuples per component than Apache Storm. Figure \ref{fig:countbolt-plot} depicts the number of tuples processed by CountBolt, the last component of the topology. Since this topology is serial, the number of tuples processed by CountBolt is a good indicator of total throughput.

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rrrrrr@{}}
    {Parallelism} & {FileReadSpout} & {SplitSentenceBolt} & {CountBolt} & {CPU Utilisation} & {Memory Usage} \\ \toprule
    1 & {25,767,502} & {25,767,502} & {225,815,174} & {217.9\%} & {690.8M} \\
    2 & {34,403,678} & {34,403,127} & {301,493,247} & {414.6\%} & {759.1M} \\
    3 & {45,731,188} & {45,732,988} & {400,767,999} & {611.5\%} & {798.4M} \\
    4 & {52,285,327} & {52,283,540} & {458,187,555} & {805.5\%} & {804.1M} \\
	5 & {55,326,941} & {55,325,167} & {484,844,652} & {998.7\%} & {806.0M} \\
	6 & {56,747,319} & {56,744,629} & {497,285,149} & {1,195.3\%} & {824.8M} \\
	10 & {40,341,798} & {40,336,962} & {353,490,567} & {1967.4\%} & {} \\
	20 & {60,798,276} & {60,790,475} & {532,737,413} & {3161.5\%} & {} \\
\end{tabular}
\caption{Storm-MC: Tuples processed per component in WordCount Topology.}
\label{table:storm_mc_wordcount}
\end{adjustwidth}
\end{table}

\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}rrrrrr@{}}
    {Parallelism} & {FileReadSpout} & {SplitSentenceBolt} & {CountBolt} & {CPU Utilisation} & {Memory Usage} \\ \toprule
    1 & {12,583,377} & {12,579,132} & {110,233,966} & {294.5\%} & {2.2G} \\
    2 & {16,800,475} & {16,796,695} & {147,194,709} & {481.7\%} & {2.8G} \\
    3 & {22,120,695} & {22,107,696} & {193,735,106} & {687.1\%} & {2.6G} \\
    4 & {20,720,637} & {20,711,756} & {181,500,586} & {895.3\%} & {2.6G} \\
	5 & {17,177,688} & {17,164,209} & {150,412,037} & {1,129.3\%} & {2.5G} \\
	6 & {17,402,418} & {17,388,691} & {152,374,303} & {1,342.1\%} & {2.3G} \\
	10 & {} & {} & {} & {\%} & {} \\
	20 & {} & {} & {} & {\%} & {} \\
\end{tabular}
\caption{Apache Storm: Tuples processed per component in WordCount Topology.}
\label{table:storm_wordcount}
\end{adjustwidth}
\end{table}

<<countbolt-plot, echo=FALSE, cache=TRUE, fig.cap="Tuples processed by CountBolt in Storm-MC and Apache Storm", fig.align="center", fig.pos="htb!", fig.height=3.5>>=
@


When the topology was run on Apache Storm in local mode, the process executed with 55 threads. Compared to that, running it on Storm-MC required only 5 threads: the main thread (1), one thread for each component (3), and a user timer used for topology metrics and ticks (1).


<<threads-plot, echo=FALSE, cache=TRUE, fig.cap="Number of threads used by Storm and Storm-MC", fig.align="center", fig.height=3.5>>=
@

\subsection{Enron Topology}

Next, we tested the Enron topology from the IBM benchmarks. In this topology, serialised emails from the Enron email database are read from a file by a spout. They are further deserialised by one bolt, filtered by another bolt, modified by yet another bolt and then finally metrics are recorded by another bolt.

Similarly, to the WordCount topology this topology is serial in nature. However, whereas the WordCount topology keeps the random sentences in memory, the Enron topology reads from a file. Thus, this benchmark is mostly I/O intensive.


\medskip
\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}llllll@{}}
    {Parallelism} & {Email Throughput} & {CPU Utilisation} & {Memory Usage} \\ \toprule
    1 & {10,868.89} & {297.7\%} & {806.8M} \\
    2 & {22,150.44} & {482.1\%} & {756.1M} \\
    3 & {28,079.70} & {729.5\%} & {341.4M} \\
    4 & {36,717.50} & {1036.9\%} & {326.0M} \\
    5 & {41,788.59} & {1311.0\%} & {260.8M} \\
    6 & {46,595.78} & {1590.3\%} & {334.0M} \\
\end{tabular}
\caption{Storm-MC: Email Throughput in Enron Topology.}
\end{adjustwidth}
\label{table:storm_mc_enron}
\end{table}
\medskip	


\medskip
\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}llllll@{}}
    {Parallelism} & {Email Throughput} & {CPU Utilisation} & {Memory Usage} \\ \toprule
    1 & {9,624.53} & {406.6\%} & {1.94G} \\
    2 & {15,897.62} & {945.1\%} & {2.93G} \\
    3 & {18,481.66} & {1,427.4\%} & {3.32G} \\
    4 & {20,517.13} & {1,891.2\%} & {3.56G} \\
    5 & {20,091.93} & {2167.4\%} & {3.65G} \\
    6 & {23,769.80} & {2388.6\%} & {4.09G} \\
\end{tabular}
\caption{Apache Storm: Email Throughput in Enron Topology.}
\end{adjustwidth}
\label{table:storm_enron}
\end{table}
\medskip

\subsection{RollingSort Topology}

The RollingSort topology is ported over from the aforementioned storm-benchmark project. This topology includes one spout and one bolt. The spout produces hundred character-long strings of random digits from zero to eight. The bolt stores a rolling window of hundred of these messages and sorts them every \textbf{x} seconds.

This benchmark is included because it is considered memory-intensive.

\medskip
\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}llllll@{}}
    {Parallelism} & {RandomMessageSpout} & {SortBolt} & {CPU Utilisation} & {Memory Usage} \\ \toprule
    1 & {249,143,444} & {249,142,400} & {186.2\%} & {504.3M} \\
    2 & {444,261,351} & {444,259,400} & {352.0\%} & {401.7M} \\
    3 & {350,861,061} & {350,859,800} & {514.7\%} & {382.9M} \\
    4 & {412,429,850} & {412,428,600} & {675.2\%} & {314.2M} \\
    5 & {470,813,184} & {470,811,300} & {835.8\%} & {423.2M} \\
    6 & {498,957,255} & {498,954,600} & {989.6\%} & {235.1M} \\
\end{tabular}
\caption{Storm-MC: Tuples processed per component in RollingSort Topology.}
\end{adjustwidth}
\label{table:storm_mc_rolling}
\end{table}
\medskip	


\medskip
\begin{table}[!htb]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{@{}llllll@{}}
    {Parallelism} & {RandomMessageSpout} & {SortBolt} & {CPU Utilisation} & {Memory Usage} \\ \toprule
    1 & {173,906,935} & {173,900,300} & {267.3\%} & {3.0G} \\
    2 & {226,583,924} & {226,579,200} & {468.3\%} & {3.0G} \\
    3 & {310,949,455} & {310,943,000} & {634.6\%} & {2.9G} \\
    4 & {362,675,336} & {362,663,600} & {815.2\%} & {2.8G} \\
    5 & {409,470,032} & {409,462,100} & {969.4\%} & {2.7G} \\
    6 & {435,471,042} & {435,459,600} & {1139.6\%} & {2.6G} \\
\end{tabular}
\caption{Apache Storm: Tuples processed per component in RollingSort Topology.}
\end{adjustwidth}
\label{table:storm_rolling}
\end{table}
\medskip

\todo{change x depending on the actual benchmark.}

\section{Challenges}
\label{sec:challenges}

In this section we are going to discuss challenges we encountered while porting Apache Storm to multi-core machines.

\begin{description}
	\item[Unfamiliarity with Clojure] \hfill \\
	
	\item[Lack of Documentation] \hfill \\
	
\end{description}

