\chapter{Introduction}

\section{Motivation}

In recent years, there has been an explosion of cloud computing software. After Google published their paper on MapReduce \citep{dean2010mapreduce}, many new open-source frameworks for distributed computation have emerged, most notably Apache Hadoop \citep{ApacheHadoop} for batch processing and Apache Storm \citep{ApacheStorm} for real-time data stream processing.

These frameworks split the work that needs to be carried out and distribute it across nodes of a cluster. Commercial companies and researchers have been able to utilise these frameworks and create distributed systems which can accomplish things that would not be otherwise possible. This has mostly been allowed by the low price and good horizontal scaling properties of commodity hardware.

%\todo{Quote that they are standard.}

At the same time, chip makers have been increasing the number of cores in processors and now we are at a point where servers with 10-core processors are standard. Moreover, most high-end servers support multiple processor sockets thus furthering the parallelisation possible with a single machine even more.

The price of multi-core servers has been going down as well. In 2008, a typical Hadoop node had two dual-core processors and 4 GB of random access memory (RAM). Nowadays, a server with two eight-core processors and 256 GB of RAM can be purchased for roughly \$10,000 USD \citep{Kumar:2013:HSD:2536274.2536314}. Hence a single server today might have better processing power than a small cluster from a few years ago \citep{Kumar:2013:HSD:2536274.2536314}. If this trend continues there will be processors with even more cores in the near future.

Moreover, tiled processors have emerged as competitors to traditional processors in throughput-based computations \cite{Tilera}. These processors use a large number of tiles connected by an on-chip network and even though the single-thread performance of each tile is lower than the performance of a conventional core, the increased parallelism yields higher per-server throughput \cite{DBLP:conf/isca/Lotfi-KamranGFVKPAJIOF12}.

It is generally believed that writing parallel software is hard. The traditional techniques of message passing and shared memory require the programmer to manage the concurrency at a fairly low level. Apache Storm has become the de facto tool used in stream processing on a cluster and according to their ``Powered By" page there are tens of companies already using Storm to process their real-time data streams \cite{PoweredBy}. We believe there is a place for a real-time data stream processing system on multi-core and Storm's popularity and simple application programming interface (API) makes it the ideal candidate for porting.

\section{Main Idea}

The main idea of this report is to take the existing Apache Storm project and port it to multi-core. This is implemented in Storm-MC - a library with an API compatible with Apache Storm. Porting the Storm API allows programmers to take an existing application written with Apache Storm in mind and run it on a multi-core server. This way, we can avoid network latency and enjoy the significant performance improvements of a shared-memory environment.

\section{Structure of the Report}

The remainder of the report is structured as follows:

\begin{itemize}
	\item \textbf{Chapter 2} presents an overview of related literature and gives background on data stream processing and multi-core architectures.
	\item \textbf{Chapter 3} explains the concepts used in Apache Storm as well as the architecture of a Storm cluster.
	\item \textbf{Chapter 4} describes how Apache Storm was ported over to Storm-MC.
	\item \textbf{Chapter 5} discusses the evaluation results of Storm-MC.
	\item \textbf{Chapter 6} presents the conclusion of this report.
\end{itemize}
